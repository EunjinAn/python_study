{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch\n",
    "\n",
    "Torch is that developed Lua language changed into python language."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch : main name space, tensor, math functions.\n",
    "\n",
    "torch.autograd: automatic differential \n",
    "\n",
    "torch.nn : data structure and layers for build network\n",
    "\n",
    "torch.optim : SGD(Stochasic Gradient Descent)\n",
    "\n",
    "torch.utils : offer the data utilities\n",
    "\n",
    "torch.ONNX(Open Neural Network Exchange) : when using share the different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.2942e-15, 1.0384e-42],\n",
      "        [5.2942e-15, 1.0384e-42],\n",
      "        [1.0675e-13, 1.0384e-42],\n",
      "        [1.0675e-13, 1.0384e-42]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(4,2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8509, 0.6354],\n",
      "        [0.9326, 0.5473],\n",
      "        [0.7556, 0.4183],\n",
      "        [0.6797, 0.6097]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# date type = long, fill in 0\n",
    "\n",
    "x = torch.zeros(4,2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6744, -0.0386, -0.9620,  0.9163],\n",
      "        [-0.8388, -0.3138, -1.7464, -1.1460]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=float)\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([1,2,3])\n",
    "print(ft)\n",
    "print(ft.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], dtype=torch.int16)\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.short())\n",
    "print(ft.int())\n",
    "print(ft.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "it = torch.IntTensor([1,2,3])\n",
    "print(it.float())\n",
    "print(it.double())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Tensor\n",
    "\n",
    "using method .to , tensor can move any one(cpu, gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9514])\n",
      "0.9514175057411194\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "tensor([1.])\n",
      "tensor([0.9514])\n",
      "tensor([1.9514])\n",
      "tensor([0.9514], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "y = torch.ones_like(x, device = device)\n",
    "print(y)\n",
    "x = x.to(device)\n",
    "print(x)\n",
    "z = x+y\n",
    "print(z)\n",
    "print(x.to('cpu', torch.double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# 0D tensor\n",
    "\n",
    "t0 = torch.tensor(0)\n",
    "print(t0.ndim)\n",
    "print(t0.shape)\n",
    "print(t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 1D tensor\n",
    "\n",
    "t1 = torch.tensor([1,2,3])\n",
    "print(t1.ndim)\n",
    "print(t1.shape)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([3, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# 2D tensor\n",
    "\n",
    "t2 = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(t2.ndim)\n",
    "print(t2.shape)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([3, 3, 3])\n",
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]],\n",
      "\n",
      "        [[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]],\n",
      "\n",
      "        [[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]]])\n"
     ]
    }
   ],
   "source": [
    "# 3D tensor : stock dataset, disease dataset by time series...\n",
    "\n",
    "t3 = torch.tensor([[[1,2,3],[4,5,6],[7,8,9]], [[1,2,3],[4,5,6],[7,8,9]], [[1,2,3],[4,5,6],[7,8,9]]])\n",
    "print(t3.ndim)\n",
    "print(t3.shape)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4D tensor : color channel (samples, height, width, channel(rgb))\n",
    "# 5D tensor : video (sample, frames, height, width, channel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "a = torch.rand(1,2) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4993,  0.2241]])\n",
      "tensor([[0.4993, 0.2241]])\n",
      "tensor([[-0., 1.]])\n",
      "tensor([[-1.,  0.]])\n",
      "tensor([[-0.4993,  0.2241]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(torch.abs(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.floor(a))\n",
    "print(torch.clamp(a, -0.5, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4993,  0.2241]])\n",
      "tensor(-0.4993)\n",
      "tensor(0.2241)\n",
      "tensor(-0.1376)\n",
      "tensor(0.5115)\n",
      "tensor(-0.1119)\n",
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(torch.min(a))\n",
    "print(torch.max(a))\n",
    "print(torch.mean(a))\n",
    "print(torch.std(a))\n",
    "print(torch.prod(a))\n",
    "print(torch.unique(torch.tensor([1,2,4,3,2,1,])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2275, 0.0239],\n",
      "        [0.7238, 0.4096]])\n",
      "torch.return_types.max(\n",
      "values=tensor([0.7238, 0.4096]),\n",
      "indices=tensor([1, 1]))\n",
      "torch.return_types.max(\n",
      "values=tensor([0.2275, 0.7238]),\n",
      "indices=tensor([0, 0]))\n"
     ]
    }
   ],
   "source": [
    "# max, min : if you give an index, argmax and argmin return.\n",
    "\n",
    "x = torch.rand(2,2)\n",
    "print(x)\n",
    "print(x.max(dim=0))\n",
    "print(x.max(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2275, 0.0239],\n",
      "        [0.7238, 0.4096]])\n",
      "torch.return_types.min(\n",
      "values=tensor([0.2275, 0.0239]),\n",
      "indices=tensor([0, 0]))\n",
      "torch.return_types.min(\n",
      "values=tensor([0.0239, 0.4096]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.min(dim=0))\n",
    "print(x.min(dim=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inplace way\n",
    "\n",
    "x.copy_(y), x.t_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2939, 0.9434],\n",
      "        [0.3833, 0.3153]])\n",
      "tensor([[0.1970, 0.6013],\n",
      "        [0.7187, 0.0235]])\n",
      "tensor([[0.4908, 1.5447],\n",
      "        [1.1020, 0.3388]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "print(x)\n",
    "print(y)\n",
    "print(x.add(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2939, 0.9434],\n",
      "        [0.3833, 0.3153]])\n",
      "tensor([[0.4908, 1.5447],\n",
      "        [1.1020, 0.3388]])\n",
      "tensor([[0.7847, 2.4882],\n",
      "        [1.4853, 0.6541]])\n",
      "tensor([[0.7847, 2.4882],\n",
      "        [1.4853, 0.6541]])\n",
      "tensor([[0.2939, 0.9434],\n",
      "        [0.3833, 0.3153]])\n",
      "tensor([[0.7847, 2.4882],\n",
      "        [1.4853, 0.6541]])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)\n",
    "print(x.add(y))\n",
    "print(y.add_(x))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mul, div, matmul(dot product)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Manipulations\n",
    "\n",
    "* slicing\n",
    "* indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1,2],[3,4]])\n",
    "print(x[0,1])\n",
    "print(x[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0628,  1.5145, -0.2660, -0.8447,  0.4475],\n",
      "        [ 1.4215, -0.4408, -0.2060, -0.0882, -0.9263],\n",
      "        [ 0.5379,  0.0263, -1.6666,  0.7223,  0.7709],\n",
      "        [ 0.6136, -1.2489, -0.2304,  0.6544,  1.1714]])\n",
      "tensor([-2.0628,  1.5145, -0.2660, -0.8447,  0.4475,  1.4215, -0.4408, -0.2060,\n",
      "        -0.0882, -0.9263,  0.5379,  0.0263, -1.6666,  0.7223,  0.7709,  0.6136,\n",
      "        -1.2489, -0.2304,  0.6544,  1.1714])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,5)\n",
    "print(x)\n",
    "y = x.view(20)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "item : 텐서에 값이 단 하나라도 존재하면 숫자값을 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1310])\n",
      "1.130972981452942\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())\n",
    "print(x.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### squeeze :  vector reduction(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4190, 0.3405, 0.1598],\n",
      "         [0.5945, 0.2925, 0.6121],\n",
      "         [0.0057, 0.5270, 0.4351]]])\n",
      "torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(1,3,3)\n",
    "print(tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4190, 0.3405, 0.1598],\n",
      "        [0.5945, 0.2925, 0.6121],\n",
      "        [0.0057, 0.5270, 0.4351]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "t = tensor.squeeze()\n",
    "print(t)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4190, 0.3405, 0.1598],\n",
      "         [0.5945, 0.2925, 0.6121],\n",
      "         [0.0057, 0.5270, 0.4351]]])\n",
      "torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# unsqueeze\n",
    "\n",
    "tensor = t. unsqueeze(dim=0)\n",
    "print(tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4190],\n",
      "         [0.3405],\n",
      "         [0.1598]],\n",
      "\n",
      "        [[0.5945],\n",
      "         [0.2925],\n",
      "         [0.6121]],\n",
      "\n",
      "        [[0.0057],\n",
      "         [0.5270],\n",
      "         [0.4351]]])\n",
      "torch.Size([3, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "tensor = t. unsqueeze(dim=2)\n",
    "print(tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([1,4])\n",
    "y = torch.FloatTensor([2,5])\n",
    "z = torch.FloatTensor([3,6])\n",
    "\n",
    "print(torch.stack([x,y,z]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5906,  0.7350,  0.2643],\n",
      "         [ 0.1712, -1.6206, -0.5989],\n",
      "         [-0.7059, -0.6261, -4.1033]],\n",
      "\n",
      "        [[ 0.3489, -1.1528, -0.9685],\n",
      "         [ 2.1626, -1.2802,  0.2717],\n",
      "         [-0.2347, -1.4900, -1.5965]]])\n",
      "tensor([[[-0.5906,  0.7350,  0.2643],\n",
      "         [ 0.1712, -1.6206, -0.5989],\n",
      "         [-0.7059, -0.6261, -4.1033],\n",
      "         [ 0.3489, -1.1528, -0.9685],\n",
      "         [ 2.1626, -1.2802,  0.2717],\n",
      "         [-0.2347, -1.4900, -1.5965]]])\n",
      "tensor([[[-0.5906,  0.7350,  0.2643,  0.3489, -1.1528, -0.9685],\n",
      "         [ 0.1712, -1.6206, -0.5989,  2.1626, -1.2802,  0.2717],\n",
      "         [-0.7059, -0.6261, -4.1033, -0.2347, -1.4900, -1.5965]]])\n"
     ]
    }
   ],
   "source": [
    "# cat (concatenate) : similar stack\n",
    "\n",
    "a = torch.randn(1,3,3)\n",
    "b= torch.randn(1,3,3)\n",
    "print(torch.cat((a,b), dim = 0))\n",
    "print(torch.cat((a,b), dim = 1))\n",
    "print(torch.cat((a,b), dim = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8044, 0.3660, 0.6623, 0.3411, 0.9194, 0.0951],\n",
      "        [0.7112, 0.8663, 0.7820, 0.1303, 0.4551, 0.1317],\n",
      "        [0.2898, 0.2501, 0.4043, 0.8421, 0.3995, 0.2774]])\n",
      "tensor([[0.8044, 0.3660],\n",
      "        [0.7112, 0.8663],\n",
      "        [0.2898, 0.2501]])\n",
      "tensor([[0.6623, 0.3411],\n",
      "        [0.7820, 0.1303],\n",
      "        [0.4043, 0.8421]])\n",
      "tensor([[0.9194, 0.0951],\n",
      "        [0.4551, 0.1317],\n",
      "        [0.3995, 0.2774]])\n"
     ]
    }
   ],
   "source": [
    "# chunk : tensor divide\n",
    "\n",
    "tensor = torch.rand(3,6)\n",
    "print(tensor)\n",
    "\n",
    "t1,t2,t3 = torch.chunk(tensor, 3, dim=1)\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2449, 0.6035, 0.0429, 0.3931, 0.9625, 0.2410],\n",
      "        [0.6332, 0.6812, 0.7653, 0.3612, 0.7872, 0.8076],\n",
      "        [0.5024, 0.2274, 0.1420, 0.5750, 0.5144, 0.8667]])\n",
      "tensor([[0.2449, 0.6035, 0.0429],\n",
      "        [0.6332, 0.6812, 0.7653],\n",
      "        [0.5024, 0.2274, 0.1420]])\n",
      "tensor([[0.3931, 0.9625, 0.2410],\n",
      "        [0.3612, 0.7872, 0.8076],\n",
      "        [0.5750, 0.5144, 0.8667]])\n"
     ]
    }
   ],
   "source": [
    "# split\n",
    "\n",
    "tensor = torch.rand(3,6)\n",
    "t1,t2 = torch.split(tensor, 3, dim = 1)\n",
    "\n",
    "print(tensor)\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd(자동미분)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3,3, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.],\n",
      "        [6., 6., 6.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x+5\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y\n",
    "out = z.mean()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n",
      "tensor([[1.3333, 1.3333, 1.3333],\n",
      "        [1.3333, 1.3333, 1.3333],\n",
      "        [1.3333, 1.3333, 1.3333]])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-804.7925,  324.6097, -818.2330], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "\n",
    "y = x*2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y*2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad() : 기울기를 계산하지 않음\n",
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.1-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 11.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch==1.13.1 in c:\\users\\dicod\\anaconda3\\lib\\site-packages (from torchvision) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dicod\\anaconda3\\lib\\site-packages (from torchvision) (4.1.1)\n",
      "Requirement already satisfied: requests in c:\\users\\dicod\\anaconda3\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\dicod\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\dicod\\anaconda3\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dicod\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dicod\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dicod\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dicod\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms : when we do data preprocessing\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean = (0.5,), std = (1.0,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST(root = '/content/', train = True, download=True, transform = mnist_transform)\n",
    "testset = datasets.MNIST(root = '/content/', train = False, download=True, transform = mnist_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size = 8, shuffle = True, num_workers = 2)\n",
    "test_loader = DataLoader(testset, batch_size = 8, shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement conv net\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 5, 5) # 1 channels, 6 filters, 5x5 filter size\n",
    "        self.pool = nn.MaxPool2d(2,2) #size 2X2, stride: 2\n",
    "        self.conv2 = nn.Conv2d(6, 14, 5)\n",
    "        self.fc1 = nn.Linear(14*4*4, 120) #16*5*5= number of output channels * dimension of the output layer after feature learning; 120 manually set as outcome of the \n",
    "        self.fc2 = nn.Linear(120, 60)\n",
    "        self.fc3 = nn.Linear(60, 10) #the output dimension (in our case 10) needs to be equal to the number of classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, ) #torch.view() function freshape the input tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters of the model\n",
    "num_epochs = 4\n",
    "batch_size = 4\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() #CrossEntropyLoss already includes SoftMax\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1792 and 350x120)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dicod\\OneDrive\\Desktop\\semester_2\\Jupyter\\pytorch.ipynb Cell 69\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dicod/OneDrive/Desktop/semester_2/Jupyter/pytorch.ipynb#Y120sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dicod/OneDrive/Desktop/semester_2/Jupyter/pytorch.ipynb#Y120sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dicod/OneDrive/Desktop/semester_2/Jupyter/pytorch.ipynb#Y120sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dicod/OneDrive/Desktop/semester_2/Jupyter/pytorch.ipynb#Y120sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dicod/OneDrive/Desktop/semester_2/Jupyter/pytorch.ipynb#Y120sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dicod\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\dicod\\OneDrive\\Desktop\\semester_2\\Jupyter\\pytorch.ipynb Cell 69\u001b[0m in \u001b[0;36mConvNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dicod/OneDrive/Desktop/semester_2/Jupyter/pytorch.ipynb#Y120sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dicod/OneDrive/Desktop/semester_2/Jupyter/pytorch.ipynb#Y120sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, ) \u001b[39m#torch.view() function freshape the input tensor\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dicod/OneDrive/Desktop/semester_2/Jupyter/pytorch.ipynb#Y120sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dicod/OneDrive/Desktop/semester_2/Jupyter/pytorch.ipynb#Y120sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dicod/OneDrive/Desktop/semester_2/Jupyter/pytorch.ipynb#Y120sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x)\n",
      "File \u001b[1;32mc:\\Users\\dicod\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dicod\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1792 and 350x120)"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "        # input_layer: 1 input channels, 6 output channels, 5 kernel size\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad() #with zero_grad() we ensure that the gradients are properly reset to zero at the start of each iteration\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #wandb.log({\"loss\": loss})  uncomment this line if you want to send data to weights and biases interface\n",
    "\n",
    "\n",
    "        if (i+1) % 2000 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "            # Calculate average accuracy for every 2000 steps\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for images, labels in test_loader:\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                accuracy = correct / total\n",
    "                #wandb.log({\"accuracy\": accuracy}) uncomment this line if you want to send data to weights and biases interface\n",
    "        \n",
    "print('Finished Training')\n",
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac12b02df44263e44825969ca3be3e510c9995b917fba12c38888608dc97063f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
